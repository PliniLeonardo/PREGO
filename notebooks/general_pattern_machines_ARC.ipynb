{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2023 Google LLC. SPDX-License-Identifier: Apache-2.0"
      ],
      "metadata": {
        "id": "Ltj0f-flNAi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2023 Google LLC. SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ],
      "metadata": {
        "id": "I8NlVpAzNB2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLMs as General Pattern Machines:** ARC Benchmark\n",
        "\n",
        "We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstract Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing controller for CartPole). While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions.\n",
        "\n",
        "This colab runs GPT-3 on the ARC benchmark with consistent tokenization (described more in Sec. 4 of the main paper).\n",
        "\n",
        "### **Quick Start:**\n",
        "\n",
        "**Step 1.** Register for an [OpenAI API key](https://openai.com/blog/openai-api/) to use GPT-3 (there's a free trial) and enter it below\n",
        "\n",
        "**Step 2.** Menu > Runtime > Run all"
      ],
      "metadata": {
        "id": "zqTADtDB6zyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = \"your-api-key-here\""
      ],
      "metadata": {
        "id": "wwJDOJSz71lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup**\n",
        "\n",
        "This does a few things:\n",
        "* Installs Python packages and sets OpenAI API key.\n",
        "* Downloads the Abstract Reasoning Corpus (ARC) benchmark.\n",
        "\n",
        "**Note:** only needs a CPU (public) runtime."
      ],
      "metadata": {
        "id": "kbWMlIj7XxX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai transformers\n",
        "\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import openai\n",
        "import pickle\n",
        "from transformers import GPT2Tokenizer\n",
        "# import tiktoken  # Faster than GPT2Tokenizer.\n",
        "\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "if not os.path.exists(\"ARC\"):\n",
        "  !git clone https://github.com/fchollet/ARC"
      ],
      "metadata": {
        "id": "uI4hX8y5XzeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **API:** Large Language Models\n",
        "\n",
        "Define helper functions to call large language models and the tokenizer.\n",
        "\n",
        "**Note:** this can get expensive."
      ],
      "metadata": {
        "id": "2AoMDZ-GZxRP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-waqt2fUb9ex"
      },
      "outputs": [],
      "source": [
        "model = \"text-davinci-003\"\n",
        "token_limit = 4096\n",
        "\n",
        "def LLM(prompt, stop=None, max_tokens=256, temperature=0):\n",
        "  responses = openai.Completion.create(engine=model, prompt=prompt, max_tokens=max_tokens, temperature=temperature, stop=stop)\n",
        "  text = [response['text'] for response in responses['choices']]\n",
        "  return text\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "LLM(\"hello world!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Alphabet:** Token Set\n",
        "\n",
        "Build a fixed token set by random sampling from the LLM's token vocabulary."
      ],
      "metadata": {
        "id": "vMPLptkxatkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "item_delim = tokenizer.encode(\",\")\n",
        "row_delim = tokenizer.encode(\"\\n\")\n",
        "sample_delim = tokenizer.encode(\"---\\n\")\n",
        "\n",
        "# Handpicked: comma-separated number matrices.\n",
        "alphabet = [tokenizer.encode(\" \" + str(a))[0] for a in range(10)]\n",
        "value_to_token = lambda x: {i:a for i, a in enumerate(alphabet)}[x]\n",
        "\n",
        "# Random sampled tokens.\n",
        "# seed_offset = 0\n",
        "# np.random.seed(42 + seed_offset)\n",
        "# alphabet = [int(i) for i in np.random.randint(tokenizer.vocab_size, size=10)]\n",
        "# value_to_token = lambda x: {i:a for i, a in enumerate(alphabet)}[x]\n",
        "\n",
        "print(\"Token Set:\", {i:value_to_token(i) for i in np.arange(10)})"
      ],
      "metadata": {
        "id": "tRIQ3AZMUmpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load:** ARC Benchmark\n",
        "\n",
        "Load tasks from the ARC benchmark."
      ],
      "metadata": {
        "id": "SD_-rknea0WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def state_to_tokens(state, value_to_token_fn):\n",
        "  tokens = []\n",
        "  for row in state:\n",
        "    for i, value in enumerate(row):\n",
        "      tokens +=[value_to_token_fn(value)]\n",
        "      if i < len(row) - 1:\n",
        "        tokens += item_delim\n",
        "    tokens += row_delim\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def task_json_to_tokens(task_json, value_to_token_fn):\n",
        "\n",
        "  # Training examples.\n",
        "  train_samples = []\n",
        "  for sample in task_json[\"train\"]:\n",
        "    tokens = []\n",
        "    tokens += tokenizer.encode(\"input:\\n\")\n",
        "    tokens += state_to_tokens(sample[\"input\"], value_to_token_fn)\n",
        "    tokens += tokenizer.encode(\"output:\\n\")\n",
        "    tokens += state_to_tokens(sample[\"output\"], value_to_token_fn)\n",
        "    tokens += sample_delim\n",
        "    train_samples.append(tokens)\n",
        "\n",
        "  # Testing examples.\n",
        "  test_inputs = []\n",
        "  test_outputs = []\n",
        "  for sample in task_json[\"test\"]:\n",
        "    inputs, outputs = [], []\n",
        "    inputs += tokenizer.encode(\"input:\\n\")\n",
        "    inputs += state_to_tokens(sample[\"input\"], value_to_token_fn)\n",
        "    inputs += tokenizer.encode(\"output:\\n\")\n",
        "    test_inputs.append(inputs)\n",
        "    outputs += state_to_tokens(sample[\"output\"], value_to_token_fn)\n",
        "    test_outputs.append(outputs)\n",
        "  return train_samples, test_inputs, test_outputs"
      ],
      "metadata": {
        "id": "5rvACw0XFZWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tasks_jsons = []\n",
        "tasks_names = []\n",
        "tasks_len = []\n",
        "task_dir = \"ARC/data/training\"\n",
        "for task_file in sorted(os.listdir(task_dir)):\n",
        "  with open(os.path.join(task_dir, task_file)) as fid:\n",
        "    task_json = json.load(fid)\n",
        "  tasks_jsons.append(task_json)\n",
        "  tasks_names.append(task_file)\n",
        "  tokens, _, _ = task_json_to_tokens(task_json, value_to_token)\n",
        "  tasks_len.append(np.sum([len(sample) for sample in tokens]))\n",
        "\n",
        "task_dir = \"ARC/data/evaluation\"\n",
        "for task_file in sorted(os.listdir(task_dir)):\n",
        "  with open(os.path.join(task_dir, task_file)) as fid:\n",
        "    task_json = json.load(fid)\n",
        "  tasks_jsons.append(task_json)\n",
        "  tasks_names.append(task_file)\n",
        "  tokens, _, _ = task_json_to_tokens(task_json, value_to_token)\n",
        "  tasks_len.append(np.sum([len(sample) for sample in tokens]))\n",
        "\n",
        "sorted_task_ids = np.argsort(tasks_len)\n",
        "\n",
        "print(\"Total number of tasks:\", len(sorted_task_ids))"
      ],
      "metadata": {
        "id": "zZY7OSoHbIRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example:** ARC Problem\n",
        "\n",
        "Show the LLM prompt for an ARC problem and visualize the grids used as inputs and outputs."
      ],
      "metadata": {
        "id": "2wBok5T59kDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors = [(0, 0, 0),\n",
        "          (0, 116, 217),\n",
        "          (255, 65, 54),\n",
        "          (46, 204, 6),\n",
        "          (255, 220, 0),\n",
        "          (170, 170, 170),\n",
        "          (240, 18, 190),\n",
        "          (255, 133, 27),\n",
        "          (127, 219, 255),\n",
        "          (135, 12, 37)]\n",
        "\n",
        "def grid_to_img(grid):\n",
        "  grid = np.int32(grid)\n",
        "  scale = 10\n",
        "  img = np.zeros((grid.shape[0] * scale + 1, grid.shape[1] * scale + 1, 3), dtype=np.uint8)\n",
        "  for r in range(grid.shape[0]):\n",
        "    for c in range(grid.shape[1]):\n",
        "      img[r*scale+1:(r+1)*scale, c*scale+1:(c+1)*scale, :] = colors[grid[r, c]]\n",
        "  new_img = img.copy()\n",
        "  new_img[0::10, :, :] = np.uint8(np.round((0.7 * np.float32(img[0::10, :, :]) + 0.3 * 255)))\n",
        "  new_img[:, 0::10, :] = np.uint8(np.round((0.7 * np.float32(img[:, 0::10, :]) + 0.3 * 255)))\n",
        "  return new_img"
      ],
      "metadata": {
        "id": "qCl4heCw88MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_json = tasks_jsons[sorted_task_ids[0]]\n",
        "\n",
        "context = []\n",
        "train_xy, test_x, test_y = task_json_to_tokens(example_json, value_to_token)\n",
        "for sample in train_xy:\n",
        "  context += sample\n",
        "context += test_x[0]\n",
        "\n",
        "print(\"PROMPT:\")\n",
        "print(tokenizer.decode(context, skip_special_tokens=True))\n",
        "print(\"SOLUTION:\")\n",
        "print(tokenizer.decode(test_y[0], skip_special_tokens=True))\n",
        "\n",
        "# Show problem.\n",
        "print(\"TRAIN:\")\n",
        "for i, ex in enumerate(example_json[\"train\"]):\n",
        "  in_img = grid_to_img(ex[\"input\"])\n",
        "  out_img = grid_to_img(ex[\"output\"])\n",
        "  plt.subplot(1, 2, 1); plt.imshow(grid_to_img(ex[\"input\"]))\n",
        "  plt.subplot(1, 2, 2); plt.imshow(grid_to_img(ex[\"output\"]))\n",
        "  plt.show()\n",
        "print(\"TEST:\")\n",
        "for i, ex in enumerate(example_json[\"test\"]):\n",
        "  in_img = grid_to_img(ex[\"input\"])\n",
        "  out_img = grid_to_img(ex[\"output\"])\n",
        "  plt.subplot(1, 2, 1); plt.imshow(grid_to_img(ex[\"input\"]))\n",
        "  plt.subplot(1, 2, 2); plt.imshow(grid_to_img(ex[\"output\"]))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "orLb7781byY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluate:** ARC Benchmark\n",
        "\n",
        "Evaluate on the available 800 tasks.\n",
        "\n",
        "**Note:** LLM temperature is set to 0 (deterministic), but your results might still vary depending on stability of the API."
      ],
      "metadata": {
        "id": "A6Jz5lc5bQRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "success = {}\n",
        "for task_id in sorted_task_ids:\n",
        "  task_json, task_name = tasks_jsons[task_id], tasks_names[task_id]\n",
        "\n",
        "  # Lazy load: skip evals where we already have results.\n",
        "  if task_name in success:\n",
        "    continue\n",
        "\n",
        "  # Build context and expected output labels.\n",
        "  context = []\n",
        "  batch_prompts = []\n",
        "  batch_labels = []\n",
        "  train_xy, test_x, test_y = task_json_to_tokens(task_json, value_to_token)\n",
        "  test_num_tokens = np.max([len(x) + len(y) for x, y in zip(test_x, test_y)])\n",
        "  for sample in train_xy:\n",
        "    if len(context) + len(sample) + test_num_tokens > token_limit:  # Ensure both train and test examples can fit in the prompt.\n",
        "      break\n",
        "    context += sample\n",
        "\n",
        "  # There can be multiple test examples so put them in the same batch.\n",
        "  for x, y in zip(test_x, test_y):\n",
        "    batch_prompts.append(context + x)\n",
        "    batch_labels.append(y)\n",
        "\n",
        "  # Run LLM.\n",
        "  try:\n",
        "    stop_token = tokenizer.decode(sample_delim, skip_special_tokens=True)\n",
        "    max_tokens = int(np.max([len(y) for y in test_y])) + 10\n",
        "    batch_responses = LLM(batch_prompts, stop=stop_token, max_tokens=max_tokens, temperature=0)\n",
        "  except Exception as e:\n",
        "    print(task_name, f\"LLM failed. {e}\")\n",
        "    continue\n",
        "\n",
        "  # Check answers and save success rates.\n",
        "  success[task_name] = 0\n",
        "  for response, label in zip(batch_responses, batch_labels):\n",
        "    label_str = tokenizer.decode(label, skip_special_tokens=True)\n",
        "    is_success = label_str.strip() in response\n",
        "    success[task_name] += is_success / len(batch_labels)\n",
        "  success[task_name] = int(success[task_name] > 0.99)  # All test cases need to correct.\n",
        "\n",
        "  # Debug prints.\n",
        "  total_success = np.sum(list(success.values()))\n",
        "  print(task_name, \"Success:\", success[task_name], \"Total:\", f\"{total_success} / {len(success)}\")\n",
        "\n",
        "  # # Save results.\n",
        "  # result_file = f\"arc-{model}-alphabet-{'-'.join(map(str, alphabet))}.pkl\"\n",
        "  # with open(result_file, 'wb') as fid:\n",
        "  #   pickle.dump(success, fid, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "xmKodFcoYzUY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}