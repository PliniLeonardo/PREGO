{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs as Symbolic Pattern Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/media/hdd/usr/edo/egoProcel_mistakes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama.generation import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONS_FOLDER = \"/media/hdd/usr/edo/egoProcel_mistakes/data/mistake_jsons_split\"\n",
    "CORRECT_JSON_FOLDER = os.path.join(JSONS_FOLDER, \"correct\")\n",
    "CORRECT_JSON_FILES = os.listdir(CORRECT_JSON_FOLDER)\n",
    "MISTAKE_JSON_FOLDER = os.path.join(JSONS_FOLDER, \"mistake\")\n",
    "MISTAKE_JSON_FILES = os.listdir(MISTAKE_JSON_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        ckpt_dir: str,\n",
    "        tokenizer_path: str,\n",
    "        max_seq_len: int = 512,\n",
    "        max_batch_size: int = 6,\n",
    "    ):\n",
    "        self.generator = Llama.build(\n",
    "            ckpt_dir=ckpt_dir,\n",
    "            tokenizer_path=tokenizer_path,\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "        )\n",
    "\n",
    "    def __call__(self, dialogs, max_gen_len=None, temperature=0.6, top_p=0.9):\n",
    "        out = self.generator.chat_completion(\n",
    "            dialogs,  # type: ignore\n",
    "            max_gen_len=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/media/hdd/usr/edo/egoProcel_mistakes/notebooks/llama.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bloki/media/hdd/usr/edo/egoProcel_mistakes/notebooks/llama.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m llama \u001b[39m=\u001b[39m LLM(ckpt_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/media/ssd/usr/edo/llama/llama-2-7b-chat\u001b[39;49m\u001b[39m\"\u001b[39;49m, tokenizer_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/media/ssd/usr/edo/llama/tokenizer.model\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/media/hdd/usr/edo/egoProcel_mistakes/notebooks/llama.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bloki/media/hdd/usr/edo/egoProcel_mistakes/notebooks/llama.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, ckpt_dir: \u001b[39mstr\u001b[39m, tokenizer_path: \u001b[39mstr\u001b[39m, max_seq_len: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m, max_batch_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m6\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bloki/media/hdd/usr/edo/egoProcel_mistakes/notebooks/llama.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator \u001b[39m=\u001b[39m Llama\u001b[39m.\u001b[39;49mbuild(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bloki/media/hdd/usr/edo/egoProcel_mistakes/notebooks/llama.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         ckpt_dir\u001b[39m=\u001b[39;49mckpt_dir,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bloki/media/hdd/usr/edo/egoProcel_mistakes/notebooks/llama.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         tokenizer_path\u001b[39m=\u001b[39;49mtokenizer_path,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bloki/media/hdd/usr/edo/egoProcel_mistakes/notebooks/llama.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         max_seq_len\u001b[39m=\u001b[39;49mmax_seq_len,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bloki/media/hdd/usr/edo/egoProcel_mistakes/notebooks/llama.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         max_batch_size\u001b[39m=\u001b[39;49mmax_batch_size,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bloki/media/hdd/usr/edo/egoProcel_mistakes/notebooks/llama.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     )\n",
      "File \u001b[0;32m/media/hdd/usr/edo/egoProcel_mistakes/llama/generation.py:85\u001b[0m, in \u001b[0;36mLlama.build\u001b[0;34m(ckpt_dir, tokenizer_path, max_seq_len, max_batch_size, model_parallel_size, seed)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mBuild a Llama instance by initializing and loading a pre-trained model.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mdistributed\u001b[39m.\u001b[39mis_initialized():\n\u001b[0;32m---> 85\u001b[0m     torch\u001b[39m.\u001b[39;49mdistributed\u001b[39m.\u001b[39;49minit_process_group(\u001b[39m\"\u001b[39;49m\u001b[39mnccl\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_parallel_is_initialized():\n\u001b[1;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m model_parallel_size \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/media/hdd/miniconda3/envs/llm/lib/python3.11/site-packages/torch/distributed/c10d_logger.py:74\u001b[0m, in \u001b[0;36m_time_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     73\u001b[0m     t1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime_ns()\n\u001b[0;32m---> 74\u001b[0m     func_return \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     t2 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime_ns()\n\u001b[1;32m     77\u001b[0m     \u001b[39mif\u001b[39;00m dist\u001b[39m.\u001b[39mis_initialized():\n",
      "File \u001b[0;32m/media/hdd/miniconda3/envs/llm/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:1141\u001b[0m, in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[39mif\u001b[39;00m store \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1138\u001b[0m     rendezvous_iterator \u001b[39m=\u001b[39m rendezvous(\n\u001b[1;32m   1139\u001b[0m         init_method, rank, world_size, timeout\u001b[39m=\u001b[39mtimeout\n\u001b[1;32m   1140\u001b[0m     )\n\u001b[0;32m-> 1141\u001b[0m     store, rank, world_size \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(rendezvous_iterator)\n\u001b[1;32m   1142\u001b[0m     store\u001b[39m.\u001b[39mset_timeout(timeout)\n\u001b[1;32m   1144\u001b[0m     \u001b[39m# Use a PrefixStore to avoid accidental overrides of keys used by\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m     \u001b[39m# different systems (e.g. RPC) in case the store is multi-tenant.\u001b[39;00m\n",
      "File \u001b[0;32m/media/hdd/miniconda3/envs/llm/lib/python3.11/site-packages/torch/distributed/rendezvous.py:231\u001b[0m, in \u001b[0;36m_env_rendezvous_handler\u001b[0;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m     rank \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(query_dict[\u001b[39m\"\u001b[39m\u001b[39mrank\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    230\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     rank \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(_get_env_or_raise(\u001b[39m\"\u001b[39;49m\u001b[39mRANK\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mworld_size\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m query_dict:\n\u001b[1;32m    234\u001b[0m     world_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(query_dict[\u001b[39m\"\u001b[39m\u001b[39mworld_size\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m/media/hdd/miniconda3/envs/llm/lib/python3.11/site-packages/torch/distributed/rendezvous.py:216\u001b[0m, in \u001b[0;36m_env_rendezvous_handler.<locals>._get_env_or_raise\u001b[0;34m(env_var)\u001b[0m\n\u001b[1;32m    214\u001b[0m env_val \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(env_var, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m env_val:\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mraise\u001b[39;00m _env_error(env_var)\n\u001b[1;32m    217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m env_val\n",
      "\u001b[0;31mValueError\u001b[0m: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set"
     ]
    }
   ],
   "source": [
    "llama = LLM(\n",
    "    ckpt_dir=\"/media/ssd/usr/edo/llama/llama-2-7b-chat\",\n",
    "    tokenizer_path=\"/media/ssd/usr/edo/llama/tokenizer.model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esempio di utilizzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nusar-2021_action_both_9026-c06e_9026_user_id_2021-02-03_170116.json\n"
     ]
    }
   ],
   "source": [
    "selected_json = CORRECT_JSON_FILES[random.randint(0, len(CORRECT_JSON_FILES))]\n",
    "\n",
    "with open(os.path.join(JSONS_FOLDER, selected_json), \"r\") as f:\n",
    "    curr_dict = json.load(f)\n",
    "input_for_LLM = curr_dict[\"context_str\"] + curr_dict[\"input_str\"]\n",
    "predicted = LLM(input_for_LLM)\n",
    "print(input_for_LLM)\n",
    "print(\"GT:\\n\", curr_dict[\"output_str\"])\n",
    "print(\"Predicted:\\n\", predicted[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "For the time being, we cannot freely use the `openai API`, we are instead constrined on their rate limits:\n",
    "- 3 PROMPTS/MIN\n",
    "- 200 PROMPTS/DAY\n",
    "\n",
    "This is why we use `time.sleep(60)` and break the evaluation at the 50® step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on all procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "correct = 0\n",
    "\n",
    "for n, json_file in enumerate(ALL_MISTAKE_JSONS[:150]):\n",
    "    tot += 1\n",
    "    with open(os.path.join(JSONS_FOLDER, json_file), \"r\") as f:\n",
    "        curr_dict = json.load(f)\n",
    "    input_for_LLM = curr_dict[\"context_str\"] + curr_dict[\"input_str\"]\n",
    "    predicted = LLM(input_for_LLM)\n",
    "    predicted = predicted[0].strip()\n",
    "    gt = curr_dict[\"output_str\"].strip()\n",
    "    print(\n",
    "        \"Procedure Label: {}\\nGT: {}\\nPred:{}\\nCorrect: {}\\n\".format(\n",
    "            curr_dict[\"procedure_label\"], gt, predicted, gt == predicted\n",
    "        )\n",
    "    )\n",
    "    if predicted == curr_dict[\"output_str\"].strip():\n",
    "        correct += 1\n",
    "\n",
    "ratio = correct / tot\n",
    "print(\"Ratio:\", ratio, f\"{correct}/{tot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on correct procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "correct = 0\n",
    "\n",
    "for n, json_file in enumerate(CORRECT_JSON_FILES):\n",
    "    tot += 1\n",
    "    print(json_file)\n",
    "    with open(os.path.join(CORRECT_JSON_FOLDER, json_file), \"r\") as f:\n",
    "        curr_dict = json.load(f)\n",
    "    input_for_LLM = curr_dict[\"context_str\"] + curr_dict[\"input_str\"]\n",
    "    predicted = LLM(input_for_LLM)\n",
    "    predicted = predicted[0].strip()\n",
    "    gt = curr_dict[\"output_str\"].strip()\n",
    "    print(\n",
    "        \"Procedure Label: {}\\nGT: {}\\nPred:{}\\nCorrect: {}\\n\".format(\n",
    "            curr_dict[\"procedure_label\"], gt, predicted, gt == predicted\n",
    "        )\n",
    "    )\n",
    "    if predicted == curr_dict[\"output_str\"].strip():\n",
    "        correct += 1\n",
    "\n",
    "ratio = correct / tot\n",
    "print(\"Ratio:\", ratio, f\"{correct}/{tot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on mistaken procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "correct = 0\n",
    "\n",
    "for n, json_file in enumerate(MISTAKE_JSON_FILES):\n",
    "    tot += 1\n",
    "    with open(os.path.join(MISTAKE_JSON_FOLDER, json_file), \"r\") as f:\n",
    "        curr_dict = json.load(f)\n",
    "    input_for_LLM = curr_dict[\"context_str\"] + curr_dict[\"input_str\"]\n",
    "    predicted = LLM(input_for_LLM)\n",
    "    predicted = predicted[0].strip()\n",
    "    gt = curr_dict[\"output_str\"].strip()\n",
    "    print(\n",
    "        \"Procedure Label: {}\\nGT: {}\\nPred:{}\\nCorrect: {}\\n\".format(\n",
    "            curr_dict[\"procedure_label\"], gt, predicted, gt == predicted\n",
    "        )\n",
    "    )\n",
    "    if predicted == curr_dict[\"output_str\"].strip():\n",
    "        correct += 1\n",
    "\n",
    "ratio = correct / tot\n",
    "print(\"Ratio:\", ratio, f\"{correct}/{tot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformers GPT-2 \n",
    "We experienced with GPT-2 from transformers, but the results are not consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "model = \"gpt2\"  # 'gpt2-medium', 'gpt2-large', 'gpt2-xl'\n",
    "max_token_produced = 100\n",
    "num_returned_sequences = 1\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model)\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def transformers_LLM(prompt):\n",
    "    return generator(\n",
    "        prompt,\n",
    "        max_length=max_token_produced,\n",
    "        num_return_sequences=num_returned_sequences,\n",
    "    )[0][\"generated_text\"].replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"input:\\n 21, 29, 107, 141, 125\\noutput:\\n 143\\n---\\ninput:\\n 143, 125, 141, 107, 29\\noutput:\\n 21\\n---\\ninput:\\n 125, 143, 29, 21, 141\\noutput:\\n\"\n",
    "print(transformers_LLM(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_strings(json_fn):\n",
    "    with open(os.path.join(JSONS_FOLDER, json_fn), \"r\") as f:\n",
    "        curr_dict = json.load(f)\n",
    "    all_truncated_prompts = []\n",
    "    all_gts = []\n",
    "    input_str = curr_dict[\"input_str\"]\n",
    "    context = curr_dict[\"context_str\"]\n",
    "    output_str = curr_dict[\"output_str\"]\n",
    "    input_prompt, sequence_, output_prompt, _ = curr_dict[\"input_str\"].split(\"\\n\")\n",
    "    sequence = sequence_.split(\",\")\n",
    "    for i in range(len(sequence)):\n",
    "        curr_str = (\n",
    "            context\n",
    "            + input_prompt\n",
    "            + \"\\n\"\n",
    "            + \",\".join(sequence[:i])\n",
    "            + \"\\n\"\n",
    "            + output_prompt\n",
    "            + \"\\n\"\n",
    "        )\n",
    "        curr_res = sequence[i]\n",
    "        all_truncated_prompts.append(curr_str)\n",
    "        all_gts.append(curr_res)\n",
    "    all_truncated_prompts.append(context + input_str)\n",
    "    all_gts.append(output_str)\n",
    "    return all_truncated_prompts, all_gts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on correct procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "correct = 0\n",
    "\n",
    "for n, json_file in enumerate(CORRECT_JSON_FILES):\n",
    "    print(json_file)\n",
    "    sequences, gts = truncated_strings(json_file)\n",
    "    for input_str, gt in zip(sequences, gts):\n",
    "        tot += 1\n",
    "        # print(\"====\\n\", input_str)\n",
    "        # print(\"GT:\", gt)\n",
    "        predicted = LLM(input_str)\n",
    "        predicted = predicted[0].strip()\n",
    "        gt = gt.strip()\n",
    "        print(\"GT: {}\\nPred:{}\\nCorrect: {}\\n\".format(gt, predicted, gt == predicted))\n",
    "        if predicted == gt:\n",
    "            correct += 1\n",
    "\n",
    "ratio = correct / tot\n",
    "print(\"Ratio:\", ratio, f\"{correct}/{tot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "correct = 0\n",
    "performance_dict_per_step = {}\n",
    "for n, json_file in enumerate(MISTAKE_JSON_FILES):\n",
    "    print(json_file)\n",
    "    sequences, gts = truncated_strings(json_file)\n",
    "    for input_str, gt in zip(sequences, gts):\n",
    "        tot += 1\n",
    "        # print(\"====\\n\", input_str)\n",
    "        # print(\"GT:\", gt)\n",
    "        predicted = LLM(input_str)\n",
    "        predicted = predicted[0].strip()\n",
    "        gt = gt.strip()\n",
    "        print(\"GT: {}\\nPred:{}\\nCorrect: {}\\n\".format(gt, predicted, gt == predicted))\n",
    "        if predicted == gt:\n",
    "            correct += 1\n",
    "\n",
    "ratio = correct / tot\n",
    "print(\"Ratio:\", ratio, f\"{correct}/{tot}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
